# Optimizers in Deep Learning

> **A quick guide to understanding what optimizers are and how they're organized**

---

## What is an Optimizer?

An **optimizer** is the algorithm that **updates the model's weights** to minimize the loss function.

### The Core Process

```
Training Loop:
1. Forward pass ‚Üí Get predictions
2. Calculate loss ‚Üí How wrong are we?
3. Backward pass ‚Üí Compute gradients (‚àÇLoss/‚àÇWeight)
4. Optimizer ‚Üí Update weights using gradients ‚Üê THIS IS THE OPTIMIZER
5. Repeat
```

**Simple analogy:**
- **Gradient** = Direction to go (uphill/downhill)
- **Optimizer** = How you walk (step size, momentum, adaptive steps)

### The Basic Update Rule

```
Weight_new = Weight_old - learning_rate √ó gradient

Where:
  - Weight_old: Current parameter value
  - gradient: How to change it (from backprop)
  - learning_rate: How big a step to take
  - Weight_new: Updated parameter value
```

---

## Why Do We Need Different Optimizers?

**The problem:** Gradient descent is simple but has issues:

```
Issues with basic gradient descent:
‚ùå Gets stuck in local minima
‚ùå Slow convergence
‚ùå Same learning rate for all parameters
‚ùå Sensitive to learning rate choice
‚ùå Oscillates in valleys

Different optimizers solve different problems!
```

---

## Optimizer Taxonomy

```
‚îú‚îÄ‚îÄ Optimizer
‚îÇ     ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ SGD family               [Momentum-based]
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ SGD                  Basic gradient descent
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ Momentum             Add velocity (physics-based)
‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ Nesterov             Look-ahead momentum
‚îÇ     ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Adaptive                 [Automatic learning rate adjustment]
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ Adam                 Most popular default
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ AdamW                Adam + proper weight decay (BERT/GPT)
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ RMSProp              Good for RNNs
‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ AdaGrad              Rarely used (LR decays too fast)
‚îÇ     ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Modern                   [Latest research, 2020+]
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ Lion                 Simpler than Adam
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ Adafactor            Memory-efficient (T5)
‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ LAMB                 Large-batch training
‚îÇ     ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Weight Decay             [Regularization]
‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ L2 penalty on weights (prevents overfitting)
‚îÇ     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ Learning Rate Schedulers ‚≠ê‚≠ê‚≠ê [Dynamic LR adjustment]
‚îÇ           ‚îú‚îÄ‚îÄ StepLR             Drop LR every N epochs
‚îÇ           ‚îú‚îÄ‚îÄ CosineAnnealingLR  üî• Smooth decay (most popular)
‚îÇ           ‚îú‚îÄ‚îÄ OneCycleLR         üî• Super-convergence
‚îÇ           ‚îú‚îÄ‚îÄ ReduceLROnPlateau  Drop when stuck
‚îÇ           ‚îî‚îÄ‚îÄ Warmup variants    ‚≠ê Essential for Transformers
```

---

## Visual: Optimizer Families

### **1. SGD Family (Momentum-Based)**

```
Idea: Build up velocity like a ball rolling downhill

    SGD (Basic)
       ‚Üì
    Add momentum ‚Üí SGD + Momentum
       ‚Üì
    Look ahead ‚Üí Nesterov Momentum

Characteristics:
  ‚úì Simple and fast
  ‚úì Works well with large learning rates
  ‚úó Requires careful LR tuning
  ‚úó Same LR for all parameters
```

**Visual representation:**
```
SGD path:
    ‚ï±‚ï≤  ‚ï±‚ï≤  ‚ï±‚ï≤      (zig-zag, slow)
   ‚ï±  ‚ï≤‚ï±  ‚ï≤‚ï±  ‚ï≤

Momentum path:
    ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤   (smooth, faster)
   ‚ï±             ‚ï≤
```

---

### **2. Adaptive Family (Per-Parameter Learning Rates)**

```
Idea: Different learning rates for each parameter

    AdaGrad (historical, too aggressive)
       ‚Üì
    RMSProp (fixes AdaGrad decay)
       ‚Üì
    Adam (RMSProp + Momentum) ‚Üê Most popular
       ‚Üì
    AdamW (Adam + proper weight decay) ‚Üê Current best practice

Characteristics:
  ‚úì Adapts to each parameter
  ‚úì Less sensitive to initial LR
  ‚úì Works out-of-the-box
  ‚úó More memory (stores moving averages)
  ‚úó Can overfit without weight decay
```

**Visual representation:**
```
Parameter space with different curvature:

Flat dimension: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (needs large LR)
Steep dimension: ‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ (needs small LR)

Adaptive optimizers adjust automatically!
```

---

### **3. Modern Family (Research & Specialized)**

```
Lion (2023):      Simpler than Adam, sign-based updates
Adafactor (2018): Memory-efficient for huge models (T5)
LAMB (2019):      Large-batch training (BERT-scale)

When to use:
  ‚Üí Cutting-edge research
  ‚Üí Specific constraints (memory, batch size)
  ‚Üí Most people stick to AdamW
```

---

### **4. Weight Decay (Regularization)**

```
Idea: Penalize large weights

Weight_new = Weight_old - lr √ó gradient - lr √ó weight_decay √ó Weight_old
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          Pulls weights toward zero

Why:
  ‚úì Prevents overfitting
  ‚úì Simpler models (smaller weights)
  
Typical values: 0.01, 0.001, 0.0001
```

**Visual:**
```
Without weight decay:    With weight decay:
  Weights can be large      Weights stay small
  ‚¨§‚¨§‚¨§‚¨§‚¨§‚¨§‚¨§‚¨§                  ‚ö´‚ö´‚ö´‚ö´
  Might overfit             Better generalization
```

---

### **5. Learning Rate Schedulers (Dynamic Adjustment)**

```
Problem: Fixed learning rate is suboptimal
  ‚Üí Start high (fast learning)
  ‚Üí End low (fine-tuning)

Solution: Adjust LR during training
```

**Visual: LR over time**

```
StepLR:
LR ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epochs
   (Drops every N epochs)

CosineAnnealingLR:
LR ‚îÇ‚ï≤
   ‚îÇ ‚ï≤___
   ‚îÇ     ‚ï≤___
   ‚îÇ         ‚ï≤___
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epochs
   (Smooth cosine decay)

OneCycleLR:
LR ‚îÇ    ‚ï±‚ï≤
   ‚îÇ   ‚ï±  ‚ï≤
   ‚îÇ  ‚ï±    ‚ï≤___
   ‚îÇ ‚ï±         ‚ï≤___
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epochs
   (Up then down)

Warmup + Cosine:
LR ‚îÇ  ‚ï±‚îÄ‚ï≤
   ‚îÇ ‚ï±   ‚ï≤___
   ‚îÇ‚ï±        ‚ï≤___
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epochs
   (Gradual start + decay)
```

---

## When to Use What?

### **Quick Decision Tree**

```
Starting a new project?
‚îî‚îÄ> Use AdamW (lr=3e-4, weight_decay=0.01)
    + CosineAnnealingLR
    Works 90% of the time

Training Transformers (BERT, GPT)?
‚îî‚îÄ> AdamW + Warmup + Cosine/Linear decay
    Industry standard

Training CNNs (ResNet, EfficientNet)?
‚îî‚îÄ> AdamW + CosineAnnealingLR
    or SGD + Momentum (if you want to tune more)

Training RNNs/LSTMs?
‚îî‚îÄ> Adam or RMSProp
    Handles noisy gradients

Very large batches (>4k)?
‚îî‚îÄ> LAMB
    Designed for this

Need extreme speed?
‚îî‚îÄ> OneCycleLR with AdamW
    Fastest convergence
```

---

## The Complete Picture

### **Optimizer Evolution Timeline**

```
1950s: SGD (Basic gradient descent)
         ‚Üì
1980s: Momentum (Add velocity)
         ‚Üì
1990s: RMSProp (Adaptive per-parameter)
         ‚Üì
2014:  Adam (RMSProp + Momentum) ‚Üê Revolution!
         ‚Üì
2017:  AdamW (Proper weight decay) ‚Üê Current standard
         ‚Üì
2019:  LAMB (Large batch)
         ‚Üì
2023:  Lion (Simplification)
```

### **Modern Best Practice (2024)**

```python
# This is what most people use today:

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=3e-4,           # Default learning rate
    weight_decay=0.01  # Regularization
)

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=num_epochs
)

# For Transformers, add warmup:
from transformers import get_cosine_schedule_with_warmup
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=1000,
    num_training_steps=total_steps
)
```

---

## Comparison Table

| Family | Example | Memory | Speed | Tuning | When to Use |
|--------|---------|--------|-------|--------|-------------|
| **SGD** | SGD + Momentum | Low | Fast | Hard | CNNs if you want to tune |
| **Adaptive** | AdamW | High | Fast | Easy | Default choice (90% of cases) |
| **Modern** | Lion | Medium | Fast | Easy | Research, cutting-edge |
| **Schedulers** | CosineAnnealingLR | None | None | Easy | Always! Pair with optimizer |

---

## Key Concepts

### **1. Learning Rate (Most Important!)**

```
Too high:  Training diverges, loss ‚Üí NaN
Too low:   Training too slow, never converges
Just right: Fast and stable convergence

Typical ranges:
  AdamW:     1e-4 to 1e-3
  SGD:       1e-2 to 1e-1 (higher than adaptive)
  Transformers: 1e-5 to 5e-5
```

### **2. Adaptive vs Non-Adaptive**

```
Non-Adaptive (SGD, Momentum):
  ‚úì Simple
  ‚úì Less memory
  ‚úó Need to tune LR carefully
  
Adaptive (Adam, AdamW):
  ‚úì Auto-adjusts per parameter
  ‚úì Works out-of-box
  ‚úó More memory
```

### **3. Why Schedulers Matter**

```
Without scheduler:
  Loss: 2.5 ‚Üí 1.8 ‚Üí 1.2 ‚Üí 0.9 ‚Üí 0.7 ‚Üí 0.65 ‚Üí stuck at 0.65

With scheduler:
  Loss: 2.5 ‚Üí 1.8 ‚Üí 1.2 ‚Üí 0.9 ‚Üí 0.7 ‚Üí 0.5 ‚Üí 0.3 ‚Üê Better!
                                  ‚Üë
                          LR decreases, fine-tunes
```

---

## Summary

### **What is an Optimizer?**
The algorithm that updates weights using gradients to minimize loss.

### **Why Different Optimizers?**
Different problems need different solutions (momentum, adaptive rates, large batches).

### **What to Use?**
- **Default**: AdamW + CosineAnnealingLR
- **Transformers**: AdamW + Warmup + Cosine
- **CNNs**: AdamW or SGD + Momentum

### **Most Important Hyperparameter?**
**Learning Rate** - Get this right first, everything else is secondary.

### **Must-Have Addition?**
**Learning Rate Scheduler** - Without it, you're leaving 20-30% performance on the table!

---

## The Essential Pattern

```python
# 95% of deep learning uses this pattern:

# 1. Choose optimizer (AdamW is default)
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=3e-4,
    weight_decay=0.01
)

# 2. Choose scheduler (CosineAnnealing is popular)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=epochs
)

# 3. Training loop
for epoch in range(epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = train_step(batch)
        loss.backward()
        optimizer.step()
    
    scheduler.step()  # Update learning rate
```

**That's it! Master this pattern and you're 90% there.**

---

*Remember: Start simple (AdamW + scheduler), only optimize if needed.*