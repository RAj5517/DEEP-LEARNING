## Quick Reference Table

| Loss Function | Category | Use Case | When to Use | PyTorch |
|--------------|----------|----------|-------------|---------|
| **MSE** | Regression | General regression | Default, well-behaved data | `nn.MSELoss()` |
| **Cross Entropy** | Classification | Multi-class | 3+ mutually exclusive classes | `nn.CrossEntropyLoss()` |
| **BCE** | Classification | Binary/Multi-label | 2 classes or independent labels | `nn.BCEWithLogitsLoss()` |
| **Focal Loss** | Classification | Imbalanced | Severe class imbalance | Custom |
| **Dice Loss** | Segmentation | Medical imaging | Small objects, pixel imbalance | Custom |
| **InfoNCE** | Metric Learning | Embeddings | Self-supervised, CLIP-style | Custom |
| **KL Divergence** | Distribution | VAE, Distillation | Match distributions | `nn.KLDivLoss()` |
| **Wasserstein** | GAN | Image generation | Stable GAN training | Custom (WGAN-GP) |
| **Smooth L1** | Regression | Object detection | Robust regression | `nn.SmoothL1Loss()` |
| **CTC** | Sequence | Speech/OCR | Alignment-free sequences | `nn.CTCLoss()` |
| **Perceptual** | Image | Style/Quality | Perceptual image tasks | Custom (VGG) |
| **Triplet** | Metric | Face recognition | Learn embeddings | `nn.TripletMarginLoss()` |
| **IoU** | Detection | Bounding boxes | Object detection | Custom |
| **Hinge** | GAN/SVM | Image synthesis | High-quality GANs | Custom |

---

## Practical Tips & Common Pitfalls

### ‚ö° General Best Practices

1. **Start Simple**: Begin with MSE/Cross Entropy before exotic losses
2. **Combine Losses**: Often `loss = Œ±¬∑loss1 + Œ≤¬∑loss2` works better
3. **Scale Matters**: Normalize inputs, outputs, and loss magnitudes
4. **Monitor Components**: Log each loss term separately
5. **Validate on Metric**: Loss ‚â† performance (track accuracy, mAP, etc.)

### ‚ö†Ô∏è Common Mistakes

**Cross Entropy:**
- ‚ùå Applying softmax before `CrossEntropyLoss` (it's included!)
- ‚ùå Using one-hot targets (use class indices)
- ‚úÖ Use `label_smoothing=0.1` for regularization

**BCE:**
- ‚ùå Using `BCELoss` with logits (numerical instability)
- ‚úÖ Always use `BCEWithLogitsLoss`

**Focal Loss:**
- ‚ùå Using on balanced datasets (unnecessary)
- ‚ùå Œ≥ too high (training instability)
- ‚úÖ Start with Œ±=0.25, Œ≥=2.0

**Contrastive/InfoNCE:**
- ‚ùå Too small batch size (<32)
- ‚ùå No normalization of embeddings
- ‚úÖ Use large batches, tune temperature carefully

**Dice Loss:**
- ‚ùå Using alone (poor gradients at boundaries)
- ‚úÖ Combine with Cross Entropy (0.5 each)

**Wasserstein:**
- ‚ùå Using batch norm in critic (breaks Lipschitz)
- ‚ùå Weight clipping without gradient penalty
- ‚úÖ Use WGAN-GP or Spectral Normalization

### üéØ Loss Selection Flowchart

```
Task Type?
‚îú‚îÄ Regression
‚îÇ  ‚îú‚îÄ Clean data, no outliers ‚Üí MSE
‚îÇ  ‚îú‚îÄ Outliers present ‚Üí Huber/Smooth L1
‚îÇ  ‚îî‚îÄ Bounding boxes ‚Üí IoU Loss
‚îÇ
‚îú‚îÄ Classification
‚îÇ  ‚îú‚îÄ Binary ‚Üí BCEWithLogitsLoss
‚îÇ  ‚îú‚îÄ Multi-class balanced ‚Üí CrossEntropyLoss
‚îÇ  ‚îú‚îÄ Multi-class imbalanced ‚Üí Focal Loss
‚îÇ  ‚îî‚îÄ Multi-label ‚Üí BCEWithLogitsLoss (per label)
‚îÇ
‚îú‚îÄ Segmentation
‚îÇ  ‚îú‚îÄ Balanced ‚Üí CrossEntropyLoss
‚îÇ  ‚îú‚îÄ Imbalanced/small objects ‚Üí Dice + CE
‚îÇ  ‚îî‚îÄ Bounding boxes ‚Üí IoU variants
‚îÇ
‚îú‚îÄ Sequence
‚îÇ  ‚îú‚îÄ Fixed alignment ‚Üí CrossEntropyLoss
‚îÇ  ‚îî‚îÄ Unknown alignment ‚Üí CTC Loss
‚îÇ
‚îú‚îÄ Embedding/Similarity
‚îÇ  ‚îú‚îÄ Pairs ‚Üí Contrastive Loss
‚îÇ  ‚îú‚îÄ Triplets ‚Üí Triplet Loss
‚îÇ  ‚îî‚îÄ Multi-modal ‚Üí InfoNCE (CLIP-style)
‚îÇ
‚îî‚îÄ Generative
   ‚îú‚îÄ VAE ‚Üí Reconstruction + KL
   ‚îú‚îÄ GAN ‚Üí Wasserstein (WGAN-GP) or Hinge
   ‚îú‚îÄ Diffusion ‚Üí MSE (denoising)
   ‚îî‚îÄ Style/Quality ‚Üí Perceptual Loss
```

### üî¨ Advanced Techniques

**Loss Weighting Strategies:**
- Manual: `loss = 0.5¬∑L1 + 0.5¬∑L2`
- Uncertainty weighting: Learn weights automatically
- Dynamic weighting: Change during training (annealing)

**Hard Example Mining:**
- Focal Loss: Automatic hard example focus
- OHEM: Online Hard Example Mining
- Hard negative mining: For Triplet Loss

**Curriculum Learning:**
- Start with easy examples, gradually harder
- Loss annealing: Gradually increase difficult term

---

## Summary: What to Remember

### üî¥ Top Priority (Master These)
1. **MSE** - Your regression default
2. **Cross Entropy** - Your classification default
3. **BCE** - Binary classification standard
4. **Focal Loss** - For any imbalanced problem
5. **Dice Loss** - For any segmentation
6. **InfoNCE / Contrastive** - For modern embeddings (CLIP, SimCLR)
7. **KL Divergence** - For VAEs and distillation
8. **Wasserstein** - For stable GAN training

### üü° High Priority (Know Well)
9. **Smooth L1 / Huber** - For robust regression & detection
10. **CTC Loss** - For speech recognition & OCR
11. **Perceptual Loss** - For image quality tasks
12. **Triplet Loss** - For face recognition & embeddings
13. **IoU variants** - For bounding box regression
14. **Hinge Loss** - For modern GANs (StyleGAN)

