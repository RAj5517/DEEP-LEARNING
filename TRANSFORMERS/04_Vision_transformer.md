# Vision Transformer (ViT)

> "What if we stopped treating images as pixels â€” and treated them as words?"

---

## The Core Idea

CNNs dominated computer vision for a decade. They slide a small filter across an image, detecting edges â†’ shapes â†’ objects layer by layer.

In 2020, Google Brain asked: what if we just used a Transformer on images directly?

The problem: Transformers work on sequences of tokens. Images are 2D grids of pixels. A 224Ã—224 image has 50,176 pixels â€” way too many to attend to one by one.

**The solution: Split the image into patches. Treat each patch as a token.**

A 224Ã—224 image split into 16Ã—16 patches = **196 tokens**. Feed those 196 tokens into a standard Transformer encoder. Done.

---

## Architecture

```
Input Image (e.g. 224Ã—224Ã—3)
â”‚
â”œâ”€â”€ Split into patches             16Ã—16 patches â†’ 196 patches
â”œâ”€â”€ Flatten each patch             16Ã—16Ã—3 = 768 values per patch
â”œâ”€â”€ Linear Projection              768 â†’ embedding dim (e.g. 768)
â”‚
â”œâ”€â”€ Add [CLS] token                prepended â€” will carry classification info
â”œâ”€â”€ Add Positional Embedding       1D learnable position per patch
â”‚
â””â”€â”€ Ã— N Transformer Encoder Layers
      â”œâ”€â”€ Multi-Head Self-Attention    patches attend to each other
      â”œâ”€â”€ Add & LayerNorm
      â”œâ”€â”€ Feed Forward Network (MLP)
      â””â”€â”€ Add & LayerNorm
â”‚
â””â”€â”€ [CLS] token output
â””â”€â”€ MLP Classification Head â†’ class label
```

---

## How Patches Become Tokens

```
Original image
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ±             â”‚
â”‚                 â”‚   224Ã—224 pixels
â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“  split into 16Ã—16 patches
â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”
â”‚P1â”‚P2â”‚P3â”‚P4â”‚P5â”‚P6â”‚   each patch = one "word"
â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤
â”‚P7â”‚P8â”‚...         â”‚   196 patches total
â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜
         â†“  flatten + linear projection
[vec1, vec2, vec3, ... vec196]   â† sequence of tokens
         â†“  standard transformer encoder
[CLS] vector â†’ "Persian Cat, 94%"
```

---

## Self-Attention in Vision

Because every patch attends to every other patch, ViT naturally learns **long-range dependencies** â€” something CNNs struggle with since they only have local receptive fields.

Patch at top-left of image can directly attend to patch at bottom-right in layer 1.
CNNs would need many convolutional layers to connect such distant regions.

This gives ViT an advantage on tasks requiring **global understanding** of the image.

---

## ViT vs CNN â€” Key Differences

```
CNN                              Vision Transformer
â”‚                                â”‚
â”œâ”€â”€ Local filters (3Ã—3, 5Ã—5)    â”œâ”€â”€ Global attention (all patches)
â”œâ”€â”€ Translation equivariant      â”œâ”€â”€ No inductive bias â€” learns from data
â”œâ”€â”€ Works with small data        â”œâ”€â”€ Needs large data (or pretraining)
â”œâ”€â”€ Efficient on CPU/GPU         â”œâ”€â”€ Needs GPU/TPU for large models
â””â”€â”€ Strong inductive bias        â””â”€â”€ More flexible, better at scale
```

**Inductive bias**: CNNs "know" that nearby pixels are related. ViT has no such assumption â€” it learns spatial relationships from scratch. This means ViT needs more data, but is more flexible.

---

## Use Cases

```
Vision Transformer Use Cases
â”‚
â”œâ”€â”€ Image Classification         ImageNet Â· medical imaging Â· satellite
â”œâ”€â”€ Object Detection             DETR Â· Swin-based detectors
â”œâ”€â”€ Semantic Segmentation        SegFormer Â· Mask2Former
â”œâ”€â”€ Image Generation             DiT (Diffusion Transformer) â€” Stable Diffusion 3
â”œâ”€â”€ Video Understanding          VideoMAE Â· TimeSformer
â””â”€â”€ Medical Imaging              PathologyViT Â· RadViT Â· cell classification
```

---

## Main Models

### ViT â€” Vision Transformer (2020) â€” Google Brain
The original. Showed that a pure Transformer with no convolutions can match or beat CNNs on ImageNet â€” but only when pre-trained on very large datasets (JFT-300M, 300 million images).

- Patch size: 16Ã—16 or 32Ã—32
- Sizes: ViT-Base (86M) Â· ViT-Large (307M) Â· ViT-Huge (632M)
- Paper: https://arxiv.org/abs/2010.11929

---

### DeiT â€” Data-efficient Image Transformers (2020) â€” Facebook AI
ViT needed 300M images to train well. DeiT showed you could train on ImageNet alone (1.2M images) using **knowledge distillation** from a CNN teacher model.

Made ViT practical for researchers without massive compute budgets.
Paper: https://arxiv.org/abs/2012.12877

---

### Swin Transformer (2021) â€” Microsoft
**Shifted Window Transformer** â€” the most practically impactful ViT variant.

Key innovations:
- **Hierarchical features** â€” like CNNs, builds from small patches to larger representations
- **Local windowed attention** â€” instead of global attention, each patch attends only to its local window (7Ã—7 patches)
- **Shifted windows** â€” windows shift between layers to allow cross-window communication

Result: Much more efficient than full ViT. Became backbone of choice for detection and segmentation.
Paper: https://arxiv.org/abs/2103.14030

---

### BEiT â€” BERT Pre-Training for Image Transformers (2021) â€” Microsoft
Applied BERT's masked language modeling to images. Tokenize image into visual tokens (using DALL-E's discrete VAE), mask random patches, make model predict the visual tokens. Strong self-supervised pre-training for vision.

Paper: https://arxiv.org/abs/2106.08254

---

### MAE â€” Masked Autoencoders (2021) â€” Meta / FAIR
Even simpler: mask 75% of image patches and make the model reconstruct the original pixels. Very compute-efficient pre-training. Used as foundation for many downstream vision tasks.

Paper: https://arxiv.org/abs/2111.06377

---

### DiT â€” Diffusion Transformer (2022)
Replaced the U-Net backbone in diffusion models with a Transformer. Used as architecture in Stable Diffusion 3, SORA (video), and modern image generation pipelines.

Paper: https://arxiv.org/abs/2212.09748

---

## Key Papers

| Paper | Link |
|-------|------|
| ViT (2020) | https://arxiv.org/abs/2010.11929 |
| DeiT (2020) | https://arxiv.org/abs/2012.12877 |
| Swin Transformer (2021) | https://arxiv.org/abs/2103.14030 |
| BEiT (2021) | https://arxiv.org/abs/2106.08254 |
| MAE (2021) | https://arxiv.org/abs/2111.06377 |
| DiT (2022) | https://arxiv.org/abs/2212.09748 |