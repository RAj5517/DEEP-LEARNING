# ğŸ”„ Sequence Modeling â€” Preprocessing Pipeline

> The model doesn't read text or music.
> It predicts the next token in a sequence.
> Everything between raw data and that prediction is **sequence preprocessing**.

---

## ğŸ” Pipeline at a Glance

```
Raw Text / Music Data
   â†“
Tokenization          (split into discrete units)
   â†“
Vocabulary Mapping    (token â†’ integer index)
   â†“
Sequence Windowing    (sliding windows over encoded data)
   â†“
(Input, Target) Pairs (target = input shifted right by 1)
   â†“
Shuffle               (break ordering bias)
   â†“
Batch                 (group for parallel GPU computation)
   â†“
LSTM / Transformer
```

---

## ğŸ“‚ Structure

| File | Covers |
|---|---|
| `01_tokenization_vocab.md` | Char/word/subword/MIDI tokens, vocab building, special tokens |
| `02_sequence_windowing.md` | Sliding window, input/target pairs, TBPTT, causal mask |
| `03_shuffle_batch.md` | DataLoader, stateful batching, gradient clipping, metrics |

---

## â“ Why This Matters

| Problem | Caused By |
|---|---|
| Model predicts garbage | No vocabulary mapping â€” raw strings into model |
| Learns ordering, not patterns | No shuffle â€” sees same order every epoch |
| Exploding gradients | No gradient clipping â€” LSTMs are especially vulnerable |
| Model peeks at the future | No causal mask in Transformer |
| Inconsistent batch shapes | No drop_last â€” final batch is smaller |

---

## âš¡ When to Apply Each Step

| Step | LSTM | Transformer | Music Gen |
|---|---|---|---|
| Char tokenization | âœ… | âš ï¸ Subword preferred | âœ… |
| Vocabulary mapping | âœ… | âœ… | âœ… |
| Sliding window | âœ… | âœ… | âœ… |
| TBPTT | âœ… | âŒ | âœ… |
| Causal mask | âŒ | âœ… | âœ… |
| Shuffle | âœ… | âœ… | âœ… |
| Stateful batching | âœ… | âŒ | âœ… |
| Gradient clipping | âœ… | âš ï¸ | âœ… |

---

## ğŸ”¬ Core Idea

Sequence modeling has one objective:

**Given everything seen so far â†’ predict the next token.**

The entire preprocessing pipeline exists to create that prediction task from raw data â€” and to deliver it to the model efficiently.

---

*For deep breakdowns, math, and code â€” refer to the individual files above.*

![alt text](../../imageset/image11.png)
![alt text](../../imageset/image12.png)
![alt text](../../imageset/image13.png)