# ğŸ”„ Sequence Modeling â€” Preprocessing Pipeline

> The model doesn't read text or music.
> It predicts the next token in a sequence.
> Everything between raw data and that prediction is **sequence preprocessing**.

---

## ğŸ” Pipeline at a Glance

```
Raw Text / Music Data
   â†“
Tokenization          (split into discrete units)
   â†“
Vocabulary Mapping    (token â†’ integer index)
   â†“
Sequence Windowing    (sliding windows over encoded data)
   â†“
(Input, Target) Pairs (target = input shifted right by 1)
   â†“
Shuffle               (break ordering bias)
   â†“
Batch                 (group for parallel GPU computation)
   â†“
LSTM / Transformer
```

---

## ğŸ“‚ Structure

| File | Covers |
|---|---|
| `01_tokenization_vocab.md` | Char/word/subword/MIDI tokens, vocab building, special tokens |
| `02_sequence_windowing.md` | Sliding window, input/target pairs, TBPTT, causal mask |
| `03_shuffle_batch.md` | DataLoader, stateful batching, gradient clipping, metrics |

---

## â“ Why This Matters

| Problem | Caused By |
|---|---|
| Model predicts garbage | No vocabulary mapping â€” raw strings into model |
| Learns ordering, not patterns | No shuffle â€” sees same order every epoch |
| Exploding gradients | No gradient clipping â€” LSTMs are especially vulnerable |
| Model peeks at the future | No causal mask in Transformer |
| Inconsistent batch shapes | No drop_last â€” final batch is smaller |

---

## âš¡ When to Apply Each Step

| Step | LSTM | Transformer | Music Gen |
|---|---|---|---|
| Char tokenization | âœ… | âš ï¸ Subword preferred | âœ… |
| Vocabulary mapping | âœ… | âœ… | âœ… |
| Sliding window | âœ… | âœ… | âœ… |
| TBPTT | âœ… | âŒ | âœ… |
| Causal mask | âŒ | âœ… | âœ… |
| Shuffle | âœ… | âœ… | âœ… |
| Stateful batching | âœ… | âŒ | âœ… |
| Gradient clipping | âœ… | âš ï¸ | âœ… |

---

## ğŸ”¬ Core Idea

Sequence modeling has one objective:

**Given everything seen so far â†’ predict the next token.**

The entire preprocessing pipeline exists to create that prediction task from raw data â€” and to deliver it to the model efficiently.

---

*For deep breakdowns, math, and code â€” refer to the individual files above.*

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       RAW TEXT / MUSIC DATA              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          1. TOKENIZATION                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Character-level   â†’  ["h","e","l","l","o"]    â† LSTM standard
  â”œâ”€â”€ Word-level        â†’  ["hello","world"]
  â”œâ”€â”€ Subword (BPE)     â†’  ["hel","lo"]             â† Transformer standard
  â””â”€â”€ Music (MIDI)      â†’  [NOTE_ON, TIME_SHIFT, NOTE_OFF ...]
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚        2. VOCABULARY MAPPING             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Build vocab       â†’  all unique tokens
  â”œâ”€â”€ char2idx          â†’  {"h":0, "e":1, "l":2 ...}
  â”œâ”€â”€ idx2char          â†’  {0:"h", 1:"e", 2:"l" ...}
  â”‚
  â””â”€â”€ Special tokens
        â”œâ”€â”€ <PAD>   â†’  padding
        â”œâ”€â”€ <UNK>   â†’  unknown / OOV
        â”œâ”€â”€ <BOS>   â†’  beginning of sequence
        â””â”€â”€ <EOS>   â†’  end of sequence
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       3. SEQUENCE WINDOWING              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Sliding window  (overlapping)
  â”‚     stride = 1  â†’  maximum samples
  â”‚
  â”œâ”€â”€ Non-overlapping
  â”‚     stride = seq_len  â†’  efficient, less redundancy
  â”‚
  â”œâ”€â”€ seq_len guide
  â”‚     â”œâ”€â”€ Char LSTM      â†’  64â€“256
  â”‚     â”œâ”€â”€ Word LSTM      â†’  35â€“100
  â”‚     â””â”€â”€ Transformer    â†’  1024â€“8192
  â”‚
  â””â”€â”€ Causal Mask  (Transformers)
        upper-triangular -inf mask â†’ no future peeking
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚    4. CREATE (INPUT, TARGET) PAIRS       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚   Input  â†’  "hell"   â†’  [h, e, l, l]
  â”‚   Target â†’  "ello"   â†’  [e, l, l, o]
  â”‚
  â”‚   Target = Input shifted right by 1 position
  â”‚   Model learns:  given token[i], predict token[i+1]
  â”‚
  â””â”€â”€ TBPTT  (LSTMs on very long sequences)
        detach hidden state every N steps
        stop gradient from flowing through entire history
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚            5. SHUFFLE                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Shuffle every epoch       â†’  prevents order memorization
  â”œâ”€â”€ DataLoader(shuffle=True)  â†’  automatic
  â””â”€â”€ Skip shuffle              â†’  stateful LSTM / time series batching
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚             6. BATCH                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ batch_size    â†’  64â€“256  (standard)
  â”œâ”€â”€ drop_last     â†’  True    (uniform batch size)
  â”œâ”€â”€ pin_memory    â†’  True    (faster GPU transfer)
  â”œâ”€â”€ num_workers   â†’  4       (parallel loading)
  â”‚
  â”œâ”€â”€ Gradient Clipping  â†’  clip_grad_norm_(max_norm=1.0)
  â””â”€â”€ Gradient Accumulation  â†’  simulate larger batch on small GPU
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       LSTM / TRANSFORMER / LLM           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â””â”€â”€ Metrics
        â”œâ”€â”€ Cross-Entropy Loss   â†’  -log P(next token)
        â”œâ”€â”€ Perplexity           â†’  exp(loss)
        â””â”€â”€ Bits per character   â†’  loss / log(2)