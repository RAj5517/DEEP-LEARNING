# ğŸ§  NLP â€” Text Preprocessing

> Neural networks do not understand language.
> They understand numbers.
> Everything between raw text and model input is **NLP preprocessing**.

---

## ğŸ” Pipeline at a Glance

```
Raw Text
   â†“
Text Cleaning        (optional â€” task dependent)
   â†“
Tokenization         (split text into units)
   â†“
Vocabulary Mapping   (token â†’ integer index)
   â†“
Padding / Truncation (uniform sequence length)
   â†“
Embedding Layer      (integer â†’ dense vector)
   â†“
Model  (LSTM / Transformer / LLM)
```

---

## ğŸ“‚ Structure

| File | Covers |
|---|---|
| `01_text_cleaning.md` | Lowercasing, stopwords, stemming, when to skip |
| `02_tokenization.md` | Character, Word, BPE, WordPiece, SentencePiece |
| `03_vocab_mapping_padding.md` | tokenâ†’index, OOV, padding, truncation, attention mask |
| `04_embedding_layer.md` | Learned, GloVe, BERT, positional encoding |

---

## â“ Why This Matters

| Problem | Caused By |
|---|---|
| Model sees garbage | No cleaning â€” HTML, URLs, noise in input |
| OOV tokens everywhere | Wrong tokenizer or no subword splitting |
| Training crashes | No padding â€” variable-length sequences can't batch |
| Slow convergence | Random embeddings â€” no pretrained initialization |
| Model ignores order | No positional encoding in Transformers |

---

## âš¡ When to Apply Each Step

| Step | LSTM | BERT / GPT | Classical NLP |
|---|---|---|---|
| Text Cleaning | Light | Skip / minimal | Aggressive |
| Tokenization | Word or Subword | WordPiece / BPE | Word-level |
| Vocab Mapping | Custom vocab | Pretrained tokenizer | Custom vocab |
| Padding | Yes | Yes | Not needed |
| Embeddings | GloVe or learned | Built-in contextual | TF-IDF / BoW |

---

## ğŸ”¬ Core Idea

Raw text â†’ tokens â†’ integers â†’ vectors â†’ model.

Every step serves one purpose:
**convert human language into a form that gradient descent can learn from.**

---

*For deep breakdowns, math, and code â€” refer to the individual files above.*
