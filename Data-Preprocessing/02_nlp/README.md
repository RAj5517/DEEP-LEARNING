# ğŸ§  NLP â€” Text Preprocessing

> Neural networks do not understand language.
> They understand numbers.
> Everything between raw text and model input is **NLP preprocessing**.

---

## ğŸ” Pipeline at a Glance

```
Raw Text
   â†“
Text Cleaning        (optional â€” task dependent)
   â†“
Tokenization         (split text into units)
   â†“
Vocabulary Mapping   (token â†’ integer index)
   â†“
Padding / Truncation (uniform sequence length)
   â†“
Embedding Layer      (integer â†’ dense vector)
   â†“
Model  (LSTM / Transformer / LLM)
```

---

## ğŸ“‚ Structure

| File | Covers |
|---|---|
| `01_text_cleaning.md` | Lowercasing, stopwords, stemming, when to skip |
| `02_tokenization.md` | Character, Word, BPE, WordPiece, SentencePiece |
| `03_vocab_mapping_padding.md` | tokenâ†’index, OOV, padding, truncation, attention mask |
| `04_embedding_layer.md` | Learned, GloVe, BERT, positional encoding |

---

## â“ Why This Matters

| Problem | Caused By |
|---|---|
| Model sees garbage | No cleaning â€” HTML, URLs, noise in input |
| OOV tokens everywhere | Wrong tokenizer or no subword splitting |
| Training crashes | No padding â€” variable-length sequences can't batch |
| Slow convergence | Random embeddings â€” no pretrained initialization |
| Model ignores order | No positional encoding in Transformers |

---

## âš¡ When to Apply Each Step

| Step | LSTM | BERT / GPT | Classical NLP |
|---|---|---|---|
| Text Cleaning | Light | Skip / minimal | Aggressive |
| Tokenization | Word or Subword | WordPiece / BPE | Word-level |
| Vocab Mapping | Custom vocab | Pretrained tokenizer | Custom vocab |
| Padding | Yes | Yes | Not needed |
| Embeddings | GloVe or learned | Built-in contextual | TF-IDF / BoW |

---

## ğŸ”¬ Core Idea

Raw text â†’ tokens â†’ integers â†’ vectors â†’ model.

Every step serves one purpose:
**convert human language into a form that gradient descent can learn from.**

---

*For deep breakdowns, math, and code â€” refer to the individual files above.*

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚               RAW TEXT                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       1. TEXT CLEANING  (optional)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Lowercasing
  â”œâ”€â”€ Remove Punctuation
  â”œâ”€â”€ Remove HTML / URLs
  â”œâ”€â”€ Remove Stopwords          (traditional NLP only)
  â”œâ”€â”€ Spelling Correction
  â”œâ”€â”€ Expand Contractions
  â””â”€â”€ Stemming / Lemmatization  (traditional NLP only)
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          2. TOKENIZATION                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Character-level
  â”‚     splits into individual characters
  â”‚
  â”œâ”€â”€ Word-level
  â”‚     splits on whitespace & punctuation
  â”‚
  â””â”€â”€ Subword  â† modern standard
        â”œâ”€â”€ BPE  (Byte-Pair Encoding)   â†’ GPT family
        â”œâ”€â”€ WordPiece                   â†’ BERT family
        â””â”€â”€ SentencePiece              â†’ T5 Â· LLaMA
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚        3. VOCABULARY MAPPING             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Build vocab   token â†’ index
  â”‚     char2idx = {tok: i for i, tok in enumerate(vocab)}
  â”‚
  â”œâ”€â”€ Encode        token â†’ integer id
  â”œâ”€â”€ Decode        integer id â†’ token
  â”‚
  â””â”€â”€ Special Tokens
        â”œâ”€â”€ [PAD]  â†’ padding
        â”œâ”€â”€ [UNK]  â†’ unknown / OOV
        â”œâ”€â”€ [CLS]  â†’ sentence start   (BERT)
        â””â”€â”€ [SEP]  â†’ sentence end     (BERT)
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚      4. PADDING / TRUNCATION             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Padding       (shorter sequences)
  â”‚     â”œâ”€â”€ Post-pad  â†’  [4, 7, 2, 0, 0]   â† default
  â”‚     â””â”€â”€ Pre-pad   â†’  [0, 0, 4, 7, 2]
  â”‚
  â””â”€â”€ Truncation    (longer sequences)
        â”œâ”€â”€ From end    â†’  seq[:max_len]
        â””â”€â”€ From start  â†’  seq[-max_len:]
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          5. EMBEDDING LAYER              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Learned from scratch
  â”‚     nn.Embedding(vocab_size, embedding_dim)
  â”‚
  â”œâ”€â”€ Pretrained Static
  â”‚     â”œâ”€â”€ Word2Vec
  â”‚     â”œâ”€â”€ GloVe
  â”‚     â””â”€â”€ FastText
  â”‚
  â”œâ”€â”€ Pretrained Contextual
  â”‚     â”œâ”€â”€ BERT  (768-dim)
  â”‚     â””â”€â”€ GPT   (768â€“12288-dim)
  â”‚
  â””â”€â”€ Positional Encoding       (Transformers only)
        â”œâ”€â”€ Sinusoidal           â†’ original Transformer
        â””â”€â”€ Learned              â†’ BERT Â· GPT
                       â”‚
                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       MODEL (LSTM / Transformer / LLM)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜